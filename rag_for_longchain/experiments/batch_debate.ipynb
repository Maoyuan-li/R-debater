{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2025/5/3 21:36\n",
    "# @Author  : Maoyuan Li\n",
    "# @File    : batch_debate_generate.py\n",
    "# @Software: PyCharm\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from rag_for_longchain.retriever.faiss_retriever import retrieve_candidates\n",
    "from rag_for_longchain.generator.testgen import generate_counterargument_via_api\n",
    "\n",
    "# -------------------- 模型与分词器初始化 --------------------\n",
    "MODEL_NAME = \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# -------------------- 重用辅助函数 --------------------\n",
    "def extract_utterances(data):\n",
    "    utterances = []\n",
    "    if isinstance(data, str):\n",
    "        for line in data.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                utterances.append(line)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            utterances.extend(extract_utterances(item))\n",
    "    elif isinstance(data, dict):\n",
    "        for v in data.values():\n",
    "            utterances.extend(extract_utterances(v))\n",
    "    else:\n",
    "        try:\n",
    "            utterances.extend(extract_utterances(str(data)))\n",
    "        except:\n",
    "            pass\n",
    "    return utterances\n",
    "\n",
    "def recursive_split(text, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) <= max_length:\n",
    "        return [text]\n",
    "    mid = len(tokens) // 2\n",
    "    part1 = tokenizer.convert_tokens_to_string(tokens[:mid])\n",
    "    part2 = tokenizer.convert_tokens_to_string(tokens[mid:])\n",
    "    return recursive_split(part1, max_length) + recursive_split(part2, max_length)\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    embs = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        embs.append(emb)\n",
    "    return np.vstack(embs)\n",
    "\n",
    "def process_chunks(chunks):\n",
    "    all_texts = []\n",
    "    skill_scores = {}\n",
    "    for c in chunks:\n",
    "        all_texts.append(c[\"text\"])\n",
    "        labels = c.get(\"labels\", {})\n",
    "        for skill, info in labels.items():\n",
    "            try:\n",
    "                score = float(info.get(\"评分\", 0))\n",
    "            except:\n",
    "                score = 0\n",
    "            skill_scores[skill] = skill_scores.get(skill, 0) + score\n",
    "    if skill_scores:\n",
    "        best_skill = max(skill_scores, key=skill_scores.get)\n",
    "        best_score = skill_scores[best_skill]\n",
    "    else:\n",
    "        best_skill, best_score = None, 0\n",
    "    return {\n",
    "        \"所有文本\": all_texts,\n",
    "        \"最佳辩论技巧\": best_skill,\n",
    "        \"最高评分\": best_score\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5de03d566eed0e3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------- 单场辩论处理 --------------------\n",
    "def process_debate_file(debate_path: Path):\n",
    "    data = json.loads(debate_path.read_text(encoding=\"utf-8\"))\n",
    "    # 1. 提取 utterances → 切块 → 生成嵌入\n",
    "    utterances = extract_utterances(data)\n",
    "    chunks = []\n",
    "    for utt in utterances:\n",
    "        for piece in recursive_split(utt):\n",
    "            chunks.append(piece)\n",
    "    print('debug:chunks',chunks)\n",
    "    embeddings = generate_embeddings(chunks)\n",
    "    # 2. 检索 top_k 片段\n",
    "    best_chunks, best_labels = retrieve_candidates(embeddings, top_k=5)\n",
    "    #print('debug:best_chuns,best_labels', best_labels)\n",
    "    # 3. 按标签统计，找到“最佳辩论技巧”\n",
    "    result = process_chunks(best_chunks)\n",
    "    all_texts_variable = \"\\n\\n\".join(result[\"所有文本\"])\n",
    "    print('debug:all_texts_variable', all_texts_variable)\n",
    "    # 4. 分别生成正方和反方文本\n",
    "    outputs = {}\n",
    "    for stance in (\"pro\", \"con\"):\n",
    "        prompt_meta = {\"最佳辩论技巧\": result[\"最佳辩论技巧\"]}\n",
    "        speech = generate_counterargument_via_api(\n",
    "            best_chunks, all_texts_variable, prompt_meta, stance=stance\n",
    "        )\n",
    "        outputs[stance] = speech\n",
    "    # 5. 返回结构\n",
    "    return {\n",
    "        \"topic\": data.get(\"topic\", debate_path.stem),\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"pro\": outputs[\"pro\"],\n",
    "        \"con\": outputs[\"con\"]\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3daeb32ba14e7403"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- 主入口 --------------------\n",
    "def main(input_dir: str, output_file: str):\n",
    "    input_dir = Path(input_dir)\n",
    "    results = []\n",
    "    # 遍历一级子文件夹\n",
    "    for sub in input_dir.iterdir():\n",
    "        if not sub.is_dir():\n",
    "            continue\n",
    "        # 找到第一个非 last_two.json 的 .json\n",
    "        for file in sub.glob(\"*.json\"):\n",
    "            if file.name == \"last_two.json\":\n",
    "                continue\n",
    "            # 处理并收集\n",
    "            res = process_debate_file(file)\n",
    "            results.append(res)\n",
    "            break\n",
    "\n",
    "    # 写入最终 JSON\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"已生成汇总文件：{output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 直接写死你的文件夹路径\n",
    "    input_dir = r\"D:\\converstional_rag\\23acldata\\input_data\\LLM实验测试\"\n",
    "    output_file = r\"D:\\converstional_rag\\23acldata\\output_data\\debaterRAG\\all_debates_output.json\"\n",
    "    main(input_dir, output_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
