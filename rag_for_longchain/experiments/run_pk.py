# -*- coding: utf-8 -*-
# @Time    : 2025/7/15 21:31
# @Author  : Maoyuan Li
# @File    : run_pk.py
# @Software: PyCharm

import os
import json
import yaml
import re
import time
import requests
import numpy as np
import torch
from pathlib import Path
from copy import deepcopy
from transformers import AutoTokenizer, AutoModel
from typing import Optional

def append_jsonl(path: Path, obj: dict) -> None:
    """å‘ JSONL è¿½åŠ ä¸€è¡Œï¼Œç«‹åˆ»è½ç›˜ã€‚"""
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open('a', encoding='utf-8') as f:
        f.write(json.dumps(obj, ensure_ascii=False))
        f.write('\n')
        f.flush()  # ç«‹å³å†™ç›˜

def write_progress(progress_file: Path, idx: int, total: int, subdir: Path, extra: str = "") -> None:
    """æŠŠå½“å‰è¿›åº¦å†™åˆ°æ–‡æœ¬æ–‡ä»¶ï¼Œä¾¿äºä½ å¿«é€Ÿå®šä½ã€‚"""
    content = f"processed={idx}/{total}\ncurrent_dir={str(subdir)}\n{extra}".strip()
    progress_file.parent.mkdir(parents=True, exist_ok=True)
    progress_file.write_text(content, encoding="utf-8")

# å¦‚æœä½ çš„æ–‡ä»¶é‡Œç”¨åˆ°äº†è¿™ä¸ªå‡½æ•°ç­¾åï¼Œè¯·æŠŠ'| None'æ”¹æˆ Optional[str]
def _safe_load_model(model_name: str, local_dir: Optional[str] = None):
    try:
        tok = AutoTokenizer.from_pretrained(model_name)
        mdl = AutoModel.from_pretrained(model_name)
        return tok, mdl
    except Exception as e:
        if local_dir and os.path.isdir(local_dir):
            tok = AutoTokenizer.from_pretrained(local_dir, local_files_only=True)
            mdl = AutoModel.from_pretrained(local_dir,  local_files_only=True)
            return tok, mdl
        raise RuntimeError(
            f"[HFåŠ è½½å¤±è´¥] {model_name} ä¸‹è½½å¤±è´¥ä¸”æ— å¯ç”¨æœ¬åœ°ç›®å½•: {local_dir}\n"
            f"åŸå§‹å¼‚å¸¸: {repr(e)}"
        ) from e



# ========= è¿è¡Œç¨³å®šæ€§ï¼ˆæƒå®œï¼‰ =========
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"       # å…ˆè·‘èµ·æ¥ï¼Œé¿å… OpenMP å†²çª
os.environ["MKL_THREADING_LAYER"] = "SEQUENTIAL"  # å‡å°‘çº¿ç¨‹å±‚å†²çª

# ========= HuggingFace/ç½‘ç»œç¨³æ€è®¾ç½® =========
# æ›´å¤§çš„è¶…æ—¶ + Rust ä¼ è¾“ï¼ˆæ›´ç¨³ï¼‰
os.environ.setdefault("HF_HUB_TIMEOUT", "60")
os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")

# ä¸º huggingface åŸŸåèµ°ç›´è¿ï¼ˆå¦‚æœä½ æœ‰ç³»ç»Ÿä»£ç†ä¸”å®ƒä¸ç¨³å®šï¼Œèƒ½ç¼“è§£ TLS ä¸­æ–­ï¼‰
os.environ.setdefault("NO_PROXY", "huggingface.co,cdn-lfs.huggingface.co")

# å°†è¯ä¹¦å›ºå®šä¸º certifiï¼ˆç»•å¼€ç³»ç»Ÿè¯ä¹¦æ··ä¹±ï¼‰
try:
    import certifi
    os.environ.setdefault("REQUESTS_CA_BUNDLE", certifi.where())
except Exception:
    pass

# ï¼ˆå¯é€‰ï¼‰è®© transformers ä¼˜å…ˆè¯»æœ¬åœ°ç¼“å­˜ï¼ˆå¦‚æœä½ å·²ç»æŠŠæ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ï¼‰
# os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")

# ------------------ é…ç½®åŠ è½½ ------------------
CONFIG_FILE = Path("D:/converstional_rag/rag_for_longchain/config/config.yaml")

def load_config(path: Path) -> dict:
    try:
        return yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    except Exception:
        return {}

cfg       = load_config(CONFIG_FILE)
api_key   = cfg.get('openai', {}).get('api_key', '')
api_base  = cfg.get('openai', {}).get('api_base', '')
llm_model = cfg.get('openai', {}).get('llm_model', 'gpt-4o')

# ------------------ é€šç”¨ï¼šå®‰å…¨åŠ è½½ HF æ¨¡å‹ï¼ˆåœ¨çº¿->æœ¬åœ°å…œåº•ï¼‰ ------------------
from typing import Optional

def _safe_load_model(model_name: str, local_dir: Optional[str] = None):

    """
    å…ˆå°è¯•åœ¨çº¿åŠ è½½ï¼›å¤±è´¥åˆ™å›é€€åˆ° local_dirï¼ˆè‹¥å­˜åœ¨ï¼‰ã€‚
    è¿”å›: (tokenizer, model)
    """
    try:
        tok = AutoTokenizer.from_pretrained(model_name)
        mdl = AutoModel.from_pretrained(model_name)
        return tok, mdl
    except Exception as e:
        if local_dir and os.path.isdir(local_dir):
            tok = AutoTokenizer.from_pretrained(local_dir, local_files_only=True)
            mdl = AutoModel.from_pretrained(local_dir,  local_files_only=True)
            return tok, mdl
        # æŠ›å‡ºæ›´å‹å¥½çš„é”™è¯¯è¯´æ˜
        raise RuntimeError(
            f"[HFåŠ è½½å¤±è´¥] æ— æ³•ä¸‹è½½ {model_name} ä¸”æœªæ‰¾åˆ°å¯ç”¨çš„æœ¬åœ°ç›®å½•: {local_dir}\n"
            f"åŸå§‹å¼‚å¸¸: {repr(e)}\n"
            f"=> è§£å†³æ–¹æ¡ˆï¼šç”¨ huggingface-cli å…ˆæŠŠæ¨¡å‹æ‹‰åˆ°æœ¬åœ°ï¼Œç„¶åæŠŠ ENC_LOCAL_DIR æŒ‡å‘é‚£ä¸ªç›®å½•ã€‚"
        ) from e

# ------------------ Utterance æå– ------------------
def extract_utterances(data):
    utterances = []
    if isinstance(data, str):
        utterances += [line.strip() for line in data.splitlines() if line.strip()]
    elif isinstance(data, dict):
        for k, v in data.items():
            if k.lower() in ('utterance', 'utterances'):
                if isinstance(v, list):
                    utterances += [str(x) for x in v]
                else:
                    utterances.append(str(v))
            else:
                utterances += extract_utterances(v)
    elif isinstance(data, list):
        for item in data:
            utterances += extract_utterances(item)
    return utterances

def prepare_query(path_or_obj):
    if isinstance(path_or_obj, (str, Path)) and str(path_or_obj).lower().endswith('.json') and Path(path_or_obj).is_file():
        obj = json.loads(Path(path_or_obj).read_text(encoding="utf-8"))
    else:
        obj = path_or_obj
    return "\n".join(extract_utterances(obj))

# ------------------ Naive RAG å†…éƒ¨å‡½æ•° ------------------
def _post_with_retry(url, headers, payload, timeout=60, max_tries=2):
    last_err = None
    for i in range(max_tries):
        try:
            return requests.post(url, headers=headers, json=payload, timeout=timeout)
        except Exception as e:
            last_err = e
            # ç®€å•æŒ‡æ•°é€€é¿
            time.sleep(1.5 * (i + 1))
    raise last_err

def generate_side_via_api(title, texts, retrieved_utts, want_stance: str):
    url = api_base.rstrip('/') + '/chat/completions'
    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}'}
    meta = json.dumps({'title': title, 'stance': want_stance}, ensure_ascii=False)
    context = '\n'.join(f"Chunk {i+1}: {u}" for i, u in enumerate(retrieved_utts))
    prompt = f"""è¯·ä»…ç”Ÿæˆ{ 'æ­£æ–¹(pro)' if want_stance.lower()=='pro' else 'åæ–¹(con)' }çš„ä¸€æ®µè¾©è®ºå‘è¨€ï¼Œ<=1100å­—ã€‚
è¿”å›**ä¸¥æ ¼ JSON**ï¼š{{"title": "...", "model": "...", "speech": "..."}}ã€‚

è¾©è®ºå…ƒæ•°æ®ï¼š
{meta}

ç”¨æˆ·è¾“å…¥ï¼š
{texts}

æ£€ç´¢ææ–™ï¼š
{context}
"""

    payload = {
        'model': llm_model,
        'messages': [
            {'role': 'system', 'content': 'You are a debate assistant. Only output JSON.'},
            {'role': 'user', 'content': prompt}
        ],
        'temperature': 0.7,
        'max_tokens': 2048,
        # å¦‚æœä½ çš„ä»£ç†æ”¯æŒ OpenAI çš„ JSON æ¨¡å¼ï¼Œå¼ºçƒˆå»ºè®®å¼€å¯ â†“â†“â†“
        'response_format': {'type': 'json_object'}
    }

    resp = _post_with_retry(url, headers, payload, timeout=60, max_tries=2)
    resp.raise_for_status()
    data = resp.json()
    raw = data['choices'][0]['message']['content']

    # ä¿é™©ï¼šå…ˆå°è¯•ç›´æ¥ loadsï¼›å¤±è´¥å°±æå–é¦–ä¸ª JSON å­ä¸²å† loads
    try:
        out = json.loads(raw)
    except Exception:
        m = re.search(r'\{[\s\S]*\}', raw)  # è´ªå©ªåŒ¹é…ç¬¬ä¸€æ®µèŠ±æ‹¬å· JSON
        out = json.loads(m.group(0)) if m else {"title": title, "model": llm_model, "speech": raw.strip()}

    # è¡¥é½å­—æ®µ + å»ç©ºç™½
    out.setdefault("title", title)
    out.setdefault("model", llm_model)
    out["speech"] = (out.get("speech") or "").strip()
    return out

# ------------------ æ£€ç´¢å™¨ ------------------
class Retriever:
    def __init__(self, db_path, encoder_name, max_tokens=512, local_dir=None):
        data  = np.load(db_path, allow_pickle=True)
        files = set(data.files)
        # åŠ è½½ utterancesï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼‰
        if 'utterances' in files:
            self.utterances = data['utterances']
        elif 'utterance' in files:
            self.utterances = data['utterance']
        else:
            self.utterances = next(
                data[n] for n in data.files
                if data[n].dtype == object or data[n].dtype.kind in ('U', 'S')
            )
        # åŠ è½½ embeddingsï¼ˆæ•°å€¼æ•°ç»„ï¼‰
        if 'embeddings' in files:
            self.embeddings = data['embeddings']
        elif 'embedding' in files:
            self.embeddings = data['embedding']
        else:
            self.embeddings = next(
                data[n] for n in data.files
                if np.issubdtype(data[n].dtype, np.number)
            )
        # åˆå§‹åŒ–ç¼–ç å™¨ï¼ˆåœ¨çº¿->æœ¬åœ°å…œåº•ï¼‰
        self.tokenizer, self.model = _safe_load_model(encoder_name, local_dir=local_dir)
        self.max_tokens = max_tokens
        # å½’ä¸€åŒ–
        self.embeddings = self.embeddings.astype(float)
        norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True) + 1e-10
        self.embeddings /= norms

    def _embed(self, text):
        toks = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            max_length=self.max_tokens
        )
        with torch.no_grad():
            out = self.model(**toks)
        vec = out.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()
        return vec / (np.linalg.norm(vec) + 1e-10)

    def retrieve(self, json_path, top_k=5):
        txt = prepare_query(json_path)
        emb = self._embed(txt)
        sims = self.embeddings.dot(emb)
        idx  = np.argsort(sims)[-top_k:][::-1]
        return [self.utterances[i] for i in idx]

# ------------------ å›ºå®šç«‹åœºåŒ…è£…å™¨ ------------------
class FixedStanceModel:
    def __init__(self, model, fixed_stance):
        self._inner     = model
        self._stance    = fixed_stance.lower()
        self._orig_name = getattr(model, 'name', type(model).__name__)
    @property
    def name(self):
        return f"{self._orig_name}({self._stance})"
    def generate_utterance(self, debate_json, context, stance):
        # ç›´æ¥ç”¨å›ºå®šç«‹åœºï¼Œæ— è§†ä¼ å…¥çš„ stance
        return self._inner.generate_utterance(debate_json, context, self._stance)

# ------------------ NaiveRAGModel é€‚é… ------------------
class NaiveRAGModel:
    def __init__(self, json_path, retriever, top_k=5):
        self.json_path = json_path
        self.title     = Path(json_path).stem
        self.retriever = retriever
        self.top_k     = top_k
    @property
    def name(self):
        return "NaiveRAG"

    def generate_utterance(self, debate_json, context, stance):
        retrieved = self.retriever.retrieve(self.json_path, top_k=self.top_k)
        texts     = prepare_query(debate_json)
        out       = generate_side_via_api(self.title, texts, retrieved, want_stance=stance)
        speech    = out.get("speech", "").strip()

        # å…œåº•ï¼šå¦‚æœä»ç„¶ç©ºï¼Œå°è¯•ä¸€æ¬¡â€œå†ç”Ÿæˆâ€æç¤º
        if not speech:
            retry_prompt_suffix = "è¯·ç”¨æ›´å‡ç»ƒçš„æ–¹å¼ç»™å‡ºä¸€æ®µå®Œæ•´ã€å¯ä¸Šåœºçš„è¾©è®ºå‘è¨€ã€‚ä»ç„¶åªè¿”å› JSONã€‚"
            out2 = generate_side_via_api(self.title, texts + "\n" + retry_prompt_suffix, retrieved, want_stance=stance)
            speech = (out2.get("speech") or "").strip()

        # å†å…œåº•ï¼šä»ä¸ºç©ºï¼Œå°±è¿”å›ä¸€ä¸ªå¯è§çš„å ä½ä¿¡æ¯ï¼Œä¾¿äºå®šä½é—®é¢˜
        return speech or "[NaiveRAGæœªç”Ÿæˆæœ‰æ•ˆå†…å®¹]"

# ===== å¤–éƒ¨ä¾èµ–ï¼ˆRDebaterModel / PKManagerï¼‰ =====
# æ³¨æ„ï¼šRDebaterModel å†…éƒ¨å¦‚æœä¹Ÿä» HF ä¸‹è½½æ¨¡å‹ï¼Œ
# ä¹Ÿä¼šå—ä¸Šé¢ç¯å¢ƒå˜é‡ä¸è¯ä¹¦è®¾ç½®çš„æ­£å‘å½±å“ã€‚
from rag_for_longchain.experiments.Rdebater_for_pk import RDebaterModel
from pkmanager import PKManager

# ------------------ ä¸»æµç¨‹ ------------------
# ==== ä¸»æµç¨‹ï¼ˆ__main__ï¼‰æ›¿æ¢ä¸ºä¸‹æ–¹ç‰ˆæœ¬ ====
if __name__ == '__main__':
    INPUT_DIR    = Path(r"D:\converstional_rag\23acldata\input_data\processed_input")
    OUTPUT_DIR   = Path(r"D:\converstional_rag\23acldata\output_data\pk_result")
    OUTPUT_JSONL = OUTPUT_DIR / "debate_pk_results_gpt-4o_1.jsonl"   # æ¯åœºä¸€è¡Œ
    OUTPUT_SNAPSHOT = OUTPUT_DIR / "debate_pk_results_snapshot_1.json" # æ¯è‹¥å¹²åœºè¦†ç›–å†™ä¸€ä»½å¿«ç…§ï¼ˆå¯é€‰ï¼‰
    PROGRESS_FILE   = OUTPUT_DIR / "progress.txt"

    DB_PATH      = r"D:/converstional_rag/rag_for_longchain/experiments/structured_embeddings_for_naive_rag.npz"
    ENC_MODEL    = "hfl/chinese-roberta-wwm-ext-large"
    ENC_LOCAL_DIR = r"D:\hf_models\hfl-chinese-roberta-wwm-ext-large"  # ä½ çš„æœ¬åœ°æ¨¡å‹ç›®å½•

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # æšä¸¾æ¯”èµ›ç›®å½•
    subdirs = [d for d in INPUT_DIR.iterdir() if d.is_dir()]
    total = len(subdirs)

    # å•ä¾‹æ£€ç´¢å™¨ï¼ˆå¸¦æœ¬åœ°å…œåº•ï¼‰
    retriever = Retriever(DB_PATH, ENC_MODEL, local_dir=ENC_LOCAL_DIR)
    all_results = []  # ä»…ç”¨äºå‘¨æœŸæ€§å¿«ç…§

    # é€åœºå¤„ç†
    for idx, sub in enumerate(subdirs, start=1):
        try:
            # æ‰¾åˆ°è¯¥åœºæ¯”èµ› JSONï¼ˆæ’é™¤ last_two.jsonï¼‰
            json_files = [p for p in sub.glob("*.json") if p.name != 'last_two.json']
            if not json_files:
                msg = f"[è·³è¿‡] ç›®å½•æ— å¯ç”¨ JSON: {sub}"
                print(msg)
                write_progress(PROGRESS_FILE, idx-1, total, sub, extra=msg)
                continue
            fn = json_files[0]

            # è¯»å–åŸå§‹ JSON
            try:
                orig_json = json.loads(fn.read_text(encoding="utf-8"))
            except Exception as e:
                msg = f"[è·³è¿‡] è¯»å– JSON å¤±è´¥: {fn} | {e}"
                print(msg)
                write_progress(PROGRESS_FILE, idx-1, total, sub, extra=msg)
                continue

            # æ·±æ‹·è´ï¼Œä¿è¯æ¯åœº debate ç‹¬ç«‹
            debate_copy = deepcopy(orig_json)

            # å®ä¾‹åŒ–å„æ¨¡å‹
            rdebater = RDebaterModel(str(CONFIG_FILE))
            naive    = NaiveRAGModel(str(fn), retriever)

            # å›ºå®šç«‹åœºï¼šRDebater åªåš proï¼ŒNaiveRAG åªåš con
            pro_model = FixedStanceModel(rdebater, 'pro')
            con_model = FixedStanceModel(naive,    'con')

            # å¤šè½®å¯¹æŠ—
            manager = PKManager(pro_model=pro_model, con_model=con_model, max_rounds=3)
            full_debate = manager.run_debate(debate_copy)

            # === å…³é”®ï¼šå¢é‡å†™å…¥ ===
            record = {
                "index": idx,
                "dir": str(sub),
                "file": str(fn),
                "result": full_debate
            }
            append_jsonl(OUTPUT_JSONL, record)   # ç«‹åˆ»è½ç›˜ä¸€åœº
            all_results.append(record)

            # æ¯å¤„ç†è‹¥å¹²åœºå°±å†™ä¸€ä¸ªå¿«ç…§ï¼ˆå¯é€‰ï¼Œé˜²æ­¢åªå‰© JSONLï¼‰
            if idx % 5 == 0 or idx == total:
                OUTPUT_SNAPSHOT.write_text(
                    json.dumps(all_results, ensure_ascii=False, indent=2),
                    encoding="utf-8"
                )

            # æ›´æ–°è¿›åº¦æ–‡ä»¶
            write_progress(PROGRESS_FILE, idx, total, sub, extra="ok")

            print(f"âœ… å·²å®Œæˆç¬¬ {idx}/{total} åœºï¼š{sub.name}")

        except Exception as e:
            # æ•è·â€œè¯¥åœºæ¯”èµ›â€çš„ä»»ä½•å…³é”®å¼‚å¸¸ï¼Œå†™è¿›åº¦å¹¶ä¸­æ­¢
            err = f"[ä¸­æ­¢] ç¬¬ {idx}/{total} åœºå¤±è´¥ï¼š{sub} | {repr(e)}"
            print(err)
            write_progress(PROGRESS_FILE, idx-1, total, sub, extra=err)
            break

    print(f"ğŸ‘‰ è¿›åº¦æ–‡ä»¶: {PROGRESS_FILE}")
    print(f"ğŸ‘‰ å¢é‡ç»“æœ(JSONL): {OUTPUT_JSONL}")
    print(f"ğŸ‘‰ å¿«ç…§(JSONæ•°ç»„): {OUTPUT_SNAPSHOT}")
